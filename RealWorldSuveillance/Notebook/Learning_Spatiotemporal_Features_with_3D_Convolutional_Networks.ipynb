{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT C3D Network using TensorFlow\n",
    "- https://github.com/jinmang2/C3D-tensorflow\n",
    "- This implementation hardly affected by above github.\n",
    "- **My ultimate goal** is to read the paper, <br>understand what the author thinks, <br>learn how to actually write the implementation, <br>and learn how to use the tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spatiotemporal Features with 3D Convolutional Networks\n",
    "- Spatiotemporal Features: 3D Convolution Networks\n",
    "- Authors refer video clips with a size of c x l x h x w\n",
    "    ```\n",
    "    where \n",
    "        c: numbver of channels \n",
    "        l: length in number of frames\n",
    "        h: height of frame\n",
    "        w: width of frame\n",
    "    ```\n",
    "- Also, authors refer 3D convolution and pooling kernel size by d x k x k\n",
    "    ```\n",
    "    where\n",
    "        d: kernel temporal depth\n",
    "        k: kernel spatial size\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "NUM_CLASSES = 101           # The UCF-101 dataset has 101 classes\n",
    "CROP_SIZE = 112             # Images are cropped to (CROP_SIZE, CROP_SIZE)\n",
    "CHANNELS = 3                # RGB Channels\n",
    "NUM_FRAMES_PER_CLIP = 16    # Number of frames per video clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Features with 3D ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 3D convolution and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(name, l_input, w, b):\n",
    "    \"\"\"Convolution layer\"\"\"\n",
    "    # All of these convolution layers are applied with appropriate padding\n",
    "    # (both spatial and temporal) and stride 1, thus there is no change in\n",
    "    # term of size from the input to the output of these convolution layers.\n",
    "    return tf.nn.bias_add(tf.nn.conv3d(input=l_input, \n",
    "                                       filter=w, \n",
    "                                       strides=[1, 1, 1, 1, 1], \n",
    "                                       padding='SAME', \n",
    "                                       name=name), \n",
    "                          b)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    \"\"\"Pooling layer\"\"\"\n",
    "    # All pooling layers are max pooling with kernel size 2 X 2 X 2 (except\n",
    "    # for the first layer) with stride 1 which means the size of output\n",
    "    # signal is reduced by a factor of 8 compared with the input signal.\n",
    "    #\n",
    "    # The first pooling layer has kernel size 1 X 2 X 2 with the intention\n",
    "    # of not to merge the temporal signal too early and also to satisfy the\n",
    "    # clip length of 16 frames (e.g. we can temporally pool with factor 2\n",
    "    # at most 4 times before completely collapsing the temporal signal).\n",
    "    #\n",
    "    # The intention of preserving the temporal information in the early phase.\n",
    "    return tf.nn.max_pool3d(l_input, ksize=[1, k, 2, 2, 1], \n",
    "                            strides=[1, k, 2, 2, 1], padding='SAME', \n",
    "                            name=name)\n",
    "\n",
    "def inference_c3d(_X, _dropout, batch_size, _weights, _biases):\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv1 = conv3d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1, name='relu1')\n",
    "    pool1 = max_pool('pool1', conv1, k=1)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv2 = conv3d('conv2', pool1, _weights['wc2'], _biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2, name='relu2')\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv3d('conv3a', pool2, _weights['wc3a'], _biases['bc3a'])\n",
    "    conv3 = tf.nn.relu(conv3, name='relu3a')\n",
    "    conv3 = conv3d('conv3b', conv3, _weights['wc3b'], _biases['bc3b'])\n",
    "    conv3 = tf.nn.relu(conv3, name='relu3b')\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv4 = conv3d('conv4a', pool3, _weights['wc4a'], _biases['bc4a'])\n",
    "    conv4 = tf.nn.relu(conv4, name='relu4a')\n",
    "    conv4 = conv3d('conv4b', conv4, _weights['wc4b'], _biases['bc4b'])\n",
    "    conv4 = tf.nn.relu(conv4, name='relu4b')\n",
    "    pool4 = max_pool('pool4', conv4, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv5 = conv3d('conv5a', pool4, _weights['wc5a'], _biases['bc5a'])\n",
    "    conv5 = tf.nn.relu(conv5, name='relu5a')\n",
    "    conv5 = conv3d('conv5b', conv5, _weights['wc5b'], _biases['bc5b'])\n",
    "    conv5 = tf.nn.relu(conv5, name='relu5b')\n",
    "    pool5 = max_pool('pool5', conv5, k=2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    pool5 = tf.transpose(pool5, perm=[0, 1, 4, 2, 3])\n",
    "    # Reshape conv3 output to fit dense layer input\n",
    "    dense1 = tf.reshape(pool5, [batch_size, _weights['wd1'].get_shape().as_list()[0]])\n",
    "    dense1 = tf.matmul(dense1, _weights['wd1']) + _biases['bd1']\n",
    "    \n",
    "    dense1 = tf.nn.relu(dense1, name='fc1') # Relu activation\n",
    "    dense1 = tf.nn.dropout(dense1, _dropout)\n",
    "    \n",
    "    dense2 = tf.nn.relu(\n",
    "        tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], \n",
    "        name='fc2') # Relu activation\n",
    "    dense2 = tf.nn.dropout(dense2, _dropout)\n",
    "    \n",
    "    # Output: class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two fully connected layers has 2,048 outputs.<br>Authors train the networks from scratch using mini-batches of 30 clips, with initial learning rate of 0.003.<br>The learning rate is divided by 10 after every 4 epochs..<br>The training is stopped after 16 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this study authors are mainly interested in,\n",
    "### How to aggregate temporal information thorough the deep networks.\n",
    "좋은 3D ConvNet architecture를 얻기 위해, 저자는 convolution layers의 다른 설정을 유지한 채, kernel temporal depth d_i를 다르게 test했다고한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying network architectures\n",
    "저자는 아래 두 가지 아키텍쳐를 실험했다.\n",
    "1. Homogeneous temporal depth\n",
    "    - All convolution layers has the same kernel temporal depth\n",
    "    - Experiment with 4 networks having kernel temporal depth of d equal to 1, 3, 5, 7\n",
    "    - Authors name these networks as **depth-d**, where _d_ is their homogeneous temporal depth.\n",
    "    - Note that _depth-1_ net is equivalent to applying 2D convolutions on separate frames.\n",
    "2. Varying temporal depth\n",
    "    - Kernel temporal depth is changing across the layers.\n",
    "    - Experiment two networks with temporal depth as followings;\n",
    "        - increasing: 3-3-5-5-7\n",
    "        - decreasing: 7-5-5-3-3\n",
    "        - from the first to the fitth convolution layer respectively\n",
    "    - Note that all of these networks have the same size of the output signal at the last pooling layer\n",
    "    - Thus, they have the same number of parameters for fully connected layers.\n",
    "    - Their number of parameters is noly different at convolution layers due to different kernel temporal depth.\n",
    "    - These differences are quite minute compared to millions of parameters in the fully connected layers.\n",
    "    - The learning capacity of the networks are comparable and the diffenences in number of parameters should not affect the results of out architecture search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exploring kernel temporal depth\n",
    "저자는 UCF101의 train split 1으로 학습을 시켰다. (kernel temporal depth)<br>\n",
    "해당 결과는 아래의 plot과 같다.<br>\n",
    "![title](https://icbcbicc.github.io/img/1.JPG)<br>\n",
    "왼쪽의 plot은 동일한(homogeneous) temporl depth를 가지는 networks의 결과를,<br>\n",
    "오른쪽의 plot은 kernel temporal depth를 변화시킨 networks의 결과를 시각화한 것이다.\n",
    "1. *homo_depth-3*의 performance가 제일 좋았다.\n",
    "2. *homo_depth-1*는 각각의 frame에 2D convolution을 적용한 것과 동일하다고 말했다.<br>\n",
    "    위의 motion modeling의 부재 때문에(sequential 고려 X) 성능이 좋게 나오지 않았다고 밝힌다.<br>\n",
    "\n",
    "또한 저자는 bigger spatial receptive field(e.g. 5X5) 와/혹은 전체 해상도(full resolution, 240X320 fram inputs)으로 실험하고 유사 동작을 관찰했다.<br>\n",
    "이에 대한 결과로 저자는 3X3X3이 3DConvNets의 최상의 kernel이라고 제시하고 비디오 분류 문제에서 2DConv보다 더 우월한 성능을 제공한다고 말한다.<br>\n",
    "또한 I380K라 불러닌 large-scale internal dataset에서 3DConvNet이 2DConvNet보다 더 우월한 성능을 보임을 검증했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Spatiotemporal feature learning\n",
    "#### Network architecture\n",
    "위의 `inference_c3d` function 참고\n",
    "\n",
    "#### Dataset\n",
    "Sports-1M dataset; which is currently the largest viideo classification benchmark<br>\n",
    "1.1 million sports video<br>\n",
    "Each video belongs to one of 487 sports categories<br>\n",
    "Compared with UCF101, Sports-1M has 5 times the number of categories and 100 times the number of videos\n",
    "\n",
    "#### Training\n",
    "Sports-1M 데이터가 굉장히 길기 때문에, 각 비디오별 2초 단위의 클립 5개를 추출<br>\n",
    "클립들은 128X171로 리사이즈<br>\n",
    "훈련할 때 공간적(spatial), 시간적(temporal) Jittering을 위해 input clip을 116X112X112로 cropping한다.\n",
    "- 참고: Jitter란 무엇인가?<br>\n",
    "https://m.blog.naver.com/PostView.nhn?blogId=lecroykorea&logNo=220991682118&proxyReferer=https%3A%2F%2Fwww.google.com%2F<br>\n",
    "\n",
    "또 이를 50%의 확률로 뒤집(flip)는다.<br>\n",
    "학습은 mini-batch SGD, batch size는 30<br>\n",
    "초기 learing_rate는 1e-3으로 매 150K iteration마다 1/2배 해준다.<br>\n",
    "최적화(optimization)은 1.9M iterations에서 끝 (대략 13 epochs)<br>\n",
    "C3D를 pre-training하고 I380K로 fine-tuning하여 실험한다.\n",
    "\n",
    "#### Sports-1M classification results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
