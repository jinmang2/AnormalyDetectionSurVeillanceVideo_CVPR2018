{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spatiotemporal Features with 3D Convolutional Networks\n",
    "- Spatiotemporal Features: 3D Convolution Networks\n",
    "- Authors refer video clips with a size of c x l x h x w\n",
    "    ```\n",
    "    where \n",
    "        c: numbver of channels \n",
    "        l: length in number of frames\n",
    "        h: height of frame\n",
    "        w: width of frame\n",
    "    ```\n",
    "- Also, authors refer 3D convolution and pooling kernel size by d x k x k\n",
    "    ```\n",
    "    where\n",
    "        d: kernel temporal depth\n",
    "        k: kernel spatial size\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 101           # The UCF-101 dataset has 101 classes\n",
    "CROP_SIZE = 112             # Images are cropped to (CROP_SIZE, CROP_SIZE)\n",
    "CHANNELS = 3                # RGB Channels\n",
    "NUM_FRAMES_PER_CLIP = 16    # Number of frames per video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(name, l_input, w, b):\n",
    "    \"\"\"Convolution layer\"\"\"\n",
    "    # All of these convolution layers are applied with appropriate padding\n",
    "    # (both spatial and temporal) and stride 1, thus there is no change in\n",
    "    # term of size from the input to the output of these convolution layers.\n",
    "    return tf.nn.bias_add(\n",
    "        tf.nn.conv3d(l_input, w, strides=[1, 1, 1, 1, 1], \n",
    "                     padding='SAME', name=name), \n",
    "        b)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    \"\"\"Pooling layer\"\"\"\n",
    "    # All pooling layers are max pooling with kernel size 2 X 2 X 2 (except\n",
    "    # for the first layer) with stride 1 which means the size of output\n",
    "    # signal is reduced by a factor of 8 compared with the input signal.\n",
    "    #\n",
    "    # The first pooling layer has kernel size 1 X 2 X 2 with the intention\n",
    "    # of not to merge the temporal signal too early and also to satisfy the\n",
    "    # clip length of 16 frames (e.g. we can temporally pool with factor 2\n",
    "    # at most 4 times before completely collapsing the temporal signal).\n",
    "    return tf.nn.max_pool3d(l_input, ksize=[1, k, 2, 2, 1], \n",
    "                            strides=[1, k, 2, 2, 1], padding='SAME', \n",
    "                            name=name)\n",
    "\n",
    "def inference_c3d(_X, _dropout, batch_size, _weights, _biases):\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv1 = conv3d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    conv1 = tf.nn.relu(conv1, name='relu1')\n",
    "    pool1 = max_pool('pool1', conv1, k=1)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv2 = conv3d('conv2', pool1, _weights['wc2'], _biases['bc2'])\n",
    "    conv2 = tf.nn.relu(conv2, name='relu2')\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv3d('conv3a', pool2, _weights['wc3a'], _biases['bc3a'])\n",
    "    conv3 = tf.nn.relu(conv3, name='relu3a')\n",
    "    conv3 = conv3d('conv3b', conv3, _weights['wc3b'], _biases['bc3b'])\n",
    "    conv3 = tf.nn.relu(conv3, name='relu3b')\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv4 = conv3d('conv4a', pool3, _weights['wc4a'], _biases['bc4a'])\n",
    "    conv4 = tf.nn.relu(conv4, name='relu4a')\n",
    "    conv4 = conv3d('conv4b', conv4, _weights['wc4b'], _biases['bc4b'])\n",
    "    conv4 = tf.nn.relu(conv4, name='relu4b')\n",
    "    pool4 = max_pool('pool4', conv4, k=2)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv5 = conv3d('conv5a', pool4, _weights['wc5a'], _biases['bc5a'])\n",
    "    conv5 = tf.nn.relu(conv5, name='relu5a')\n",
    "    conv5 = conv3d('conv5b', conv5, _weights['wc5b'], _biases['bc5b'])\n",
    "    conv5 = tf.nn.relu(conv5, name='relu5b')\n",
    "    pool5 = max_pool('pool5', conv5, k=2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    pool5 = tf.transpose(pool5, perm=[0, 1, 4, 2, 3])\n",
    "    dense1 = tf.reshape(pool5, [batch_size, _weights['wd1'].get_shape().as_list()[0]]) # Reshape conv3 output to fit dense layer input\n",
    "    dense1 = tf.matmul(dense1, _weights['wd1']) + _biases['bd1']\n",
    "    \n",
    "    dense1 = tf.nn.relu(dense1, name='fc1') # Relu activation\n",
    "    dense1 = tf.nn.dropout(dense1, _dropout)\n",
    "    \n",
    "    dense2 = tf.nn.relu(\n",
    "        tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], \n",
    "        name='fc2') # Relu activation\n",
    "    dense2 = tf.nn.dropout(dense2, _dropout)\n",
    "    \n",
    "    # Output: class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two fully connected layers has 2,048 outputs. Authors train the networks from scratch using mini-batches of 30 clips, with initial learning rate of 0.003. The learning rate is divided by 10 after every 4 epochs.. The training is stopped after 16 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this study authors are mainly interested in,\n",
    "### How to aggregate temporal information thorough the deep networks.\n",
    "좋은 3D ConvNet architecture를 얻기 위해, 저자는 convolution layers의 다른 설정을 유지한 채, kernel temporal depth d_i를 다르게 test했다고한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저자는 아래 두 가지 아키텍쳐를 실험했다.\n",
    "1. Homogeneous temporal depth\n",
    "    - All convolution layers has the same kernel temporal depth\n",
    "    - Experiment with 4 networks having kernel temporal depth of d equal to 1, 3, 5, 7\n",
    "    - Authors name these networks as **depth-d**, where _d_ is their homogeneous temporal depth.\n",
    "    - Note that _depth-1_ net is equivalent to applying 2D convolutions on separate frames.\n",
    "2. Varying temporal depth\n",
    "    - Kernel temporal depth is changing across the layers.\n",
    "    - Experiment two networks with temporal depth as followings;\n",
    "        - increasing: 3-3-5-5-7\n",
    "        - decreasing: 7-5-5-3-3\n",
    "        - from the first to the fitth convolution layer respectively\n",
    "    - Note that all of these networks have the same size of the output signal at the last pooling layer\n",
    "    - Thus, they have the same number of parameters for fully connected layers.\n",
    "    - Their number of parameters is noly different at convolution layers due to different kernel temporal depth.\n",
    "    - These differences are quite minute compared to millions of parameters in the fully connected layers.\n",
    "    - The learning capacity of the networks are comparable and the diffenences in number of parameters should not affect the results of out architecture search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
